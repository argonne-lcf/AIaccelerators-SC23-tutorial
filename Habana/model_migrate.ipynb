{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68877607",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.  \n",
    "Copyright (c) 2017, Pytorch contributors All rights reserved.\n",
    "## BSD 3-Clause License\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16169a66",
   "metadata": {},
   "source": [
    "# ResNet50 for PyTorch with GPU Migration\n",
    "\n",
    "In this notebook we will demonstrate ResNet50 model which is based on open source implementation of ResNet50. It has been enabled using an experimental feature called GPU migration toolkit and it can be trained using Pytorch on 8 HPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666669b",
   "metadata": {},
   "source": [
    "#### Clone the Model-References repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root\n",
    "!git clone https://github.com/habanaai/Model-References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bace79a",
   "metadata": {},
   "source": [
    "#### Set the ENV variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41f0f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=/root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision:/usr/lib/habanalabs/:/root\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH=/root/Model-References:/usr/lib/habanalabs/:/root\n",
    "%set_env PYTHON=/usr/bin/python3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64847f32",
   "metadata": {},
   "source": [
    "#### Naviagte to the model to begin the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2212748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision\n"
     ]
    }
   ],
   "source": [
    "%cd /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a208072",
   "metadata": {},
   "source": [
    "#### Download dataset\n",
    "ImageNet 2012 dataset needs to be organized according to PyTorch requirements, and as specified in the scripts of [imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee70dc",
   "metadata": {},
   "source": [
    "#### Import GPU Migration Toolkit package and Habana Torch Library\n",
    "Look into train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c905c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4\timport habana_frameworks.torch.gpu_migration\n",
      "     5\timport datetime\n",
      "     6\timport os\n",
      "     7\timport time\n",
      "     8\timport warnings\n",
      "     9\t\n",
      "    10\timport presets\n",
      "    11\timport torch\n",
      "    12\timport torch.utils.data\n",
      "    13\timport torchvision\n",
      "    14\timport transforms\n",
      "    15\timport utils\n",
      "    16\tfrom sampler import RASampler\n",
      "    17\tfrom torch import nn\n",
      "    18\tfrom torch.utils.data.dataloader import default_collate\n",
      "    19\tfrom torchvision.transforms.functional import InterpolationMode\n",
      "    20\timport habana_frameworks.torch.core as htcore\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 20 | tail -n 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed748205",
   "metadata": {},
   "source": [
    "#### Placing mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4e1aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    49\t            loss.backward()\n",
      "    50\t            htcore.mark_step()\n",
      "    51\t            if args.clip_grad_norm is not None:\n",
      "    52\t                nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
      "    53\t            optimizer.step()\n",
      "    54\t            htcore.mark_step()\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 55 | tail -n 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea291f86",
   "metadata": {},
   "source": [
    "#### Run the following command to start training on 1 HPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b92d4",
   "metadata": {},
   "source": [
    "```bash\n",
    "!GPU_MIGRATION_LOG_LEVEL=1 $PYTHON train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=/root/software/data/pytorch/imagenet/ILSVRC2012 --workers=8 --epochs=1 --opt=sgd --amp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8607d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-09/23-15-51/gpu_migration_13114.log\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-09/23-15-51/gpu_migration_13118.log\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-09/23-15-51/gpu_migration_13117.log\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-09/23-15-51/gpu_migration_13115.log\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-09/23-15-51/gpu_migration_13119.log\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-09/23-15-51/gpu_migration_13120.log\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=1, ) --> torch.hpu.set_device(hpu:1)\n",
      "\u001b[0m\n",
      "| distributed init (rank 1): env://\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=1, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=5, ) --> torch.hpu.set_device(hpu:5)\n",
      "\u001b[0m\n",
      "| distributed init (rank 5): env://\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=5, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=4, ) --> torch.hpu.set_device(hpu:4)\n",
      "\u001b[0m\n",
      "| distributed init (rank 4): env://\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=4, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-09/23-15-51/gpu_migration_13113.log\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=2, ) --> torch.hpu.set_device(hpu:2)\n",
      "\u001b[0m\n",
      "| distributed init (rank 2): env://\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=2, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=6, ) --> torch.hpu.set_device(hpu:6)\n",
      "\u001b[0m\n",
      "| distributed init (rank 6): env://\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=6, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-09/23-15-51/gpu_migration_13116.log\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=7, ) --> torch.hpu.set_device(hpu:7)\n",
      "\u001b[0m\n",
      "| distributed init (rank 7): env://\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=7, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=0, ) --> torch.hpu.set_device(hpu:0)\n",
      "\u001b[0m\n",
      "| distributed init (rank 0): env://\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=0, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=3, ) --> torch.hpu.set_device(hpu:3)\n",
      "\u001b[0m\n",
      "| distributed init (rank 3): env://\n",
      "[2023-06-09 23:15:51] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=3, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(amp=True, augmix_severity=3, auto_augment=None, batch_size=256, bias_weight_decay=None, cache_dataset=False, clip_grad_norm=None, cutmix_alpha=0.0, data_path='/root/software/data/pytorch/imagenet/ILSVRC2012', device='cuda', dist_backend='nccl', dist_url='env://', distributed=True, dl_worker_type='HABANA', epochs=1, gpu=0, interpolation='bilinear', label_smoothing=0.0, lr=0.1, lr_gamma=0.1, lr_min=0.0, lr_scheduler='custom_lr', lr_step_size=30, lr_warmup_decay=0.01, lr_warmup_epochs=0, lr_warmup_method='constant', mixup_alpha=0.0, model='resnet50', model_ema=False, model_ema_decay=0.99998, model_ema_steps=32, momentum=0.9, norm_weight_decay=None, opt='sgd', output_dir='.', print_freq=10, ra_magnitude=9, ra_reps=3, ra_sampler=False, random_erase=0.0, rank=0, resume='', seed=123, start_epoch=0, sync_bn=False, test_only=False, train_crop_size=224, transformer_embedding_decay=None, use_deterministic_algorithms=False, val_crop_size=224, val_resize_size=256, weight_decay=0.0001, weights=None, workers=8, world_size=8)\n",
      "[2023-06-09 23:15:51] /usr/local/lib/python3.8/dist-packages/torch/random.py:40\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=123, ) --> torch.hpu.random.manual_seed_all(123)\n",
      "\u001b[0m\n",
      "Loading data\n",
      "Loading training data\n",
      "Took 3.941493034362793\n",
      "Loading validation data\n",
      "Creating data loaders\n",
      "MediaDataloader 1/8 seed : 429734646\n",
      "MediaDataloader 6/8 seed : 451811581\n",
      "MediaDataloader 7/8 seed : 486685118\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 160\n",
      "CPU RAM = 1056438708 KB \n",
      "============================================================================================ \n",
      "HabanaDataLoader device type  4\n",
      "MediaDataloader 2/8 seed : 514100512\n",
      "MediaDataloader 3/8 seed : 517003002\n",
      "Warning: Updated shuffle to True as sampler is DistributedSampler with shuffle True\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x7f47c78d8f10>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform RandomResizedCrop: Random Crop,Resize w:h  224 224  scale:  (0.08, 1.0)  ratio:  (0.75, 1.3333333333333333)  interpolation:  InterpolationMode.BILINEAR\n",
      "transform RandomHorizontalFlip: probability  0.5\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 8 instance id 0\n",
      "MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name HPUMediaPipe:1\n",
      "MediaDataloader 5/8 seed : 586864143\n",
      "MediaDataloader 0/8 seed : 586988408\n",
      "Decode ResizedCrop w:h 224 224\n",
      "MediaDataloader shuffle is  True\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... Done!\n",
      "MediaDataloader 4/8 seed : 669071919\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 1273840 classes 996\n",
      "num_slices 8 slice_index 0\n",
      "random seed used  586988408\n",
      "sliced media files/labels 159230\n",
      "Finding largest file ...\n",
      "largest file is  /root/software/data/pytorch/imagenet/ILSVRC2012/train/n07717556/n07717556_14716.JPEG\n",
      "Running with Habana media DataLoader with num_instances = 8, instance_id = 0.\n",
      "MediaDataloader 7/8 seed : 245549209\n",
      "HabanaDataLoader device type  4\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x7f47b64cc5b0>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform Resize: w:h  256 256  interpolation:  InterpolationMode.BILINEAR  max_size:  None\n",
      "transform CenterCrop: w:h  224 224\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 8 instance id 0\n",
      "MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name HPUMediaPipe:2\n",
      "MediaDataloader 0/8 seed : 416561076\n",
      "Decode w:h  256 256  , Crop disabled\n",
      "MediaDataloader shuffle is  False\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... MediaDataloader 5/8 seed : 442922478\n",
      "MediaDataloader 2/8 seed : 548829829\n",
      "Done!\n",
      "MediaDataloader 6/8 seed : 564061651\n",
      "MediaDataloader 3/8 seed : 1687758718\n",
      "MediaDataloader 1/8 seed : 1700883088\n",
      "MediaDataloader 4/8 seed : 1781659799\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 50000 classes 1000\n",
      "num_slices 8 slice_index 0\n",
      "random seed used  416561076\n",
      "sliced media files/labels 6250\n",
      "Finding largest file ...\n",
      "largest file is  /root/software/data/pytorch/imagenet/ILSVRC2012/val/n01630670/ILSVRC2012_val_00046430.JPEG\n",
      "Running with Habana media DataLoader with num_instances = 8, instance_id = 0.\n",
      "Creating model\n",
      "[2023-06-09 23:17:23] /usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/core/weight_sharing.py:173\n",
      "    [context]:     result = self.original_to(*args, **kwargs)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'), None, False), kwargs={}, ) --> torch.Tensor.to(args=('hpu', None, False), kwargs={})\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:23] train.py:306\n",
      "    [context]:     scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.amp.GradScaler.__init__(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True, ) --> set enabled to Flase\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:23] train.py:349\n",
      "    [context]:         model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], broadcast_buffers=False, gradient_as_bucket_view=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.nn.parallel.DistributedDataParallel.__init__(module=module, device_ids=[0], output_device=None, dim=0, broadcast_buffers=False, process_group=None, bucket_cap_mb=25, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=True, static_graph=False, ) --> change device_ids and output_device to None\n",
      "\u001b[0m\n",
      "Start training\n",
      "[2023-06-09 23:17:25] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:113\n",
      "    [context]:         if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Shuffling ... Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Done!\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-09 23:17:26] train.py:32\n",
      "    [context]:         image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:26] train.py:33\n",
      "    [context]:         with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.autocast.__init__(device_type=cuda, dtype=torch.float16, enabled=True, cache_enabled=True, ) --> torch.autocast.__init__(device_type=hpu, dtype=None, enabled=True, cache_enabled=True, )\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:26] /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/common.py:7\n",
      "    [context]:     return not (torch.cuda.is_available() or find_spec('torch_xla'))\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:37] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:137\n",
      "    [context]:                 if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:37] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:146\n",
      "    [context]:                             memory=torch.cuda.max_memory_allocated() / MB,\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.max_memory_allocated(device=None, ) --> torch.hpu.max_memory_allocated(device=None)\n",
      "\u001b[0m\n",
      "Epoch: [0]  [  0/622]  eta: 2:06:32  lr: 0.1  img/s: 20.973556319305924  loss: 7.1048 (7.1048)  acc1: 0.3906 (0.3906)  acc5: 0.3906 (0.3906)  time: 12.2059  data: 1.2269  max mem: 15838\n",
      "Epoch: [0]  [ 10/622]  eta: 0:31:50  lr: 0.1  img/s: 115.61183054130028  loss: 7.1048 (7.1195)  acc1: 0.3906 (0.3906)  acc5: 0.3906 (0.7812)  time: 3.1221  data: 0.1968  max mem: 16966\n",
      "Epoch: [0]  [ 20/622]  eta: 0:16:40  lr: 0.1  img/s: 4432.052451134573  loss: 7.1133 (7.1174)  acc1: 0.3906 (0.3906)  acc5: 1.1719 (0.9115)  time: 1.1344  data: 0.0488  max mem: 16966\n",
      "Epoch: [0]  [ 30/622]  eta: 0:11:16  lr: 0.1  img/s: 4516.90829523116  loss: 7.1133 (7.1285)  acc1: 0.3906 (0.4883)  acc5: 0.7812 (0.8789)  time: 0.0546  data: 0.0051  max mem: 16966\n",
      "Epoch: [0]  [ 40/622]  eta: 0:08:30  lr: 0.1  img/s: 4853.195422435741  loss: 7.1133 (7.0867)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.8594)  time: 0.0521  data: 0.0047  max mem: 16966\n",
      "Epoch: [0]  [ 50/622]  eta: 0:06:48  lr: 0.1  img/s: 5000.998223147521  loss: 7.1048 (7.0657)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.8464)  time: 0.0493  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [ 60/622]  eta: 0:05:39  lr: 0.1  img/s: 5282.966572527873  loss: 7.1048 (7.0459)  acc1: 0.3906 (0.3348)  acc5: 0.7812 (0.7812)  time: 0.0472  data: 0.0015  max mem: 16966\n",
      "Epoch: [0]  [ 70/622]  eta: 0:04:50  lr: 0.1  img/s: 4933.84042494532  loss: 6.9607 (7.0270)  acc1: 0.3906 (0.2930)  acc5: 0.7812 (0.7812)  time: 0.0475  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [ 80/622]  eta: 0:04:13  lr: 0.1  img/s: 4933.219318243425  loss: 6.9607 (7.0097)  acc1: 0.3906 (0.3038)  acc5: 0.7812 (0.7812)  time: 0.0492  data: 0.0014  max mem: 16966\n",
      "Epoch: [0]  [ 90/622]  eta: 0:03:44  lr: 0.1  img/s: 5296.3007443733  loss: 6.9274 (6.9888)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.9766)  time: 0.0475  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [100/622]  eta: 0:03:20  lr: 0.1  img/s: 5273.366650950809  loss: 6.9274 (6.9741)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.9233)  time: 0.0458  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [110/622]  eta: 0:03:01  lr: 0.1  img/s: 5202.998810872521  loss: 6.9192 (6.9543)  acc1: 0.3906 (0.3581)  acc5: 0.7812 (1.0417)  time: 0.0462  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [120/622]  eta: 0:02:44  lr: 0.1  img/s: 5171.708403494103  loss: 6.9192 (6.9406)  acc1: 0.3906 (0.3606)  acc5: 0.7812 (1.1118)  time: 0.0467  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [130/622]  eta: 0:02:30  lr: 0.1  img/s: 5266.486614806604  loss: 6.8949 (6.9260)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (1.3393)  time: 0.0464  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [140/622]  eta: 0:02:20  lr: 0.1  img/s: 2805.1565217414022  loss: 6.8949 (6.9064)  acc1: 0.3906 (0.4167)  acc5: 0.7812 (1.4583)  time: 0.0673  data: 0.0086  max mem: 16966\n",
      "Epoch: [0]  [150/622]  eta: 0:02:09  lr: 0.1  img/s: 5234.665052013101  loss: 6.8713 (6.8888)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (1.4893)  time: 0.0675  data: 0.0095  max mem: 16966\n",
      "Epoch: [0]  [160/622]  eta: 0:02:00  lr: 0.1  img/s: 5222.205370535011  loss: 6.8713 (6.8671)  acc1: 0.3906 (0.4136)  acc5: 1.1719 (1.5165)  time: 0.0463  data: 0.0023  max mem: 16966\n",
      "Epoch: [0]  [170/622]  eta: 0:01:52  lr: 0.1  img/s: 5223.231671485778  loss: 6.8270 (6.8482)  acc1: 0.3906 (0.4774)  acc5: 1.1719 (1.6059)  time: 0.0463  data: 0.0014  max mem: 16966\n",
      "Epoch: [0]  [180/622]  eta: 0:01:44  lr: 0.1  img/s: 4716.0108590957225  loss: 6.8270 (6.8283)  acc1: 0.3906 (0.4729)  acc5: 1.1719 (1.6653)  time: 0.0489  data: 0.0041  max mem: 16966\n",
      "Epoch: [0]  [190/622]  eta: 0:01:38  lr: 0.1  img/s: 5201.287478013948  loss: 6.8005 (6.8103)  acc1: 0.3906 (0.5078)  acc5: 1.1719 (1.8164)  time: 0.0490  data: 0.0040  max mem: 16966\n",
      "Epoch: [0]  [200/622]  eta: 0:01:32  lr: 0.1  img/s: 5223.7449732474115  loss: 6.7767 (6.7917)  acc1: 0.3906 (0.5580)  acc5: 1.9531 (1.9903)  time: 0.0465  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [210/622]  eta: 0:01:26  lr: 0.1  img/s: 5242.224721909762  loss: 6.7362 (6.7727)  acc1: 0.3906 (0.6037)  acc5: 1.9531 (2.1662)  time: 0.0463  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [220/622]  eta: 0:01:21  lr: 0.1  img/s: 5272.3490329002  loss: 6.7361 (6.7572)  acc1: 0.3906 (0.6624)  acc5: 1.9531 (2.2418)  time: 0.0460  data: 0.0010  max mem: 16966\n",
      "Epoch: [0]  [230/622]  eta: 0:01:16  lr: 0.1  img/s: 5257.62599791603  loss: 6.6319 (6.7398)  acc1: 0.3906 (0.6673)  acc5: 2.3438 (2.2786)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [240/622]  eta: 0:01:12  lr: 0.1  img/s: 5265.554278899224  loss: 6.6249 (6.7193)  acc1: 0.7812 (0.7188)  acc5: 2.7344 (2.3438)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [250/622]  eta: 0:01:08  lr: 0.1  img/s: 5245.37462640924  loss: 6.5275 (6.6944)  acc1: 0.7812 (0.7662)  acc5: 2.7344 (2.5090)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [260/622]  eta: 0:01:04  lr: 0.1  img/s: 5223.676357764838  loss: 6.5193 (6.6699)  acc1: 0.7812 (0.7668)  acc5: 3.1250 (2.5608)  time: 0.0463  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [270/622]  eta: 0:01:01  lr: 0.1  img/s: 4976.946303265318  loss: 6.4692 (6.6476)  acc1: 0.7812 (0.7673)  acc5: 3.1250 (2.6925)  time: 0.0475  data: 0.0031  max mem: 16966\n",
      "Epoch: [0]  [280/622]  eta: 0:00:58  lr: 0.1  img/s: 4616.917909177062  loss: 6.4686 (6.6303)  acc1: 0.7812 (0.8082)  acc5: 3.1250 (2.8691)  time: 0.0506  data: 0.0069  max mem: 16966\n",
      "Epoch: [0]  [290/622]  eta: 0:00:54  lr: 0.1  img/s: 4638.213659670236  loss: 6.4193 (6.6123)  acc1: 0.7812 (0.8464)  acc5: 3.9062 (3.0990)  time: 0.0526  data: 0.0075  max mem: 16966\n",
      "Epoch: [0]  [300/622]  eta: 0:00:52  lr: 0.1  img/s: 4683.1702654478195  loss: 6.4160 (6.5903)  acc1: 0.7812 (0.9451)  acc5: 3.9062 (3.2006)  time: 0.0523  data: 0.0051  max mem: 16966\n",
      "Epoch: [0]  [310/622]  eta: 0:00:49  lr: 0.1  img/s: 5126.348193291654  loss: 6.3754 (6.5707)  acc1: 1.1719 (1.0010)  acc5: 3.9062 (3.4302)  time: 0.0497  data: 0.0023  max mem: 16966\n",
      "Epoch: [0]  [320/622]  eta: 0:00:46  lr: 0.1  img/s: 5302.313549730277  loss: 6.3376 (6.5510)  acc1: 1.5625 (1.0535)  acc5: 4.2969 (3.5748)  time: 0.0465  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [330/622]  eta: 0:00:44  lr: 0.1  img/s: 5269.187344039455  loss: 6.2278 (6.5302)  acc1: 1.5625 (1.1029)  acc5: 4.6875 (3.8143)  time: 0.0458  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [340/622]  eta: 0:00:41  lr: 0.1  img/s: 5273.364061093401  loss: 6.1463 (6.5111)  acc1: 1.5625 (1.1719)  acc5: 5.4688 (4.0290)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [350/622]  eta: 0:00:39  lr: 0.1  img/s: 5061.5752637813775  loss: 6.0904 (6.4893)  acc1: 1.5625 (1.1719)  acc5: 5.8594 (4.1775)  time: 0.0470  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [360/622]  eta: 0:00:37  lr: 0.1  img/s: 4311.871557253055  loss: 6.0738 (6.4701)  acc1: 1.9531 (1.2141)  acc5: 6.2500 (4.3919)  time: 0.0524  data: 0.0061  max mem: 16966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [370/622]  eta: 0:00:35  lr: 0.1  img/s: 4708.559641186386  loss: 6.0441 (6.4522)  acc1: 1.9531 (1.2747)  acc5: 6.2500 (4.5539)  time: 0.0542  data: 0.0080  max mem: 16966\n",
      "Epoch: [0]  [380/622]  eta: 0:00:33  lr: 0.1  img/s: 3969.727520264712  loss: 6.0329 (6.4277)  acc1: 1.9531 (1.3822)  acc5: 6.6406 (4.7676)  time: 0.0568  data: 0.0114  max mem: 16966\n",
      "Epoch: [0]  [390/622]  eta: 0:00:31  lr: 0.1  img/s: 5295.522353315996  loss: 5.9631 (6.4052)  acc1: 1.9531 (1.4258)  acc5: 7.8125 (5.0781)  time: 0.0538  data: 0.0095  max mem: 16966\n",
      "Epoch: [0]  [400/622]  eta: 0:00:29  lr: 0.1  img/s: 3096.166358897938  loss: 5.9305 (6.3836)  acc1: 1.9531 (1.4863)  acc5: 8.2031 (5.3163)  time: 0.0628  data: 0.0150  max mem: 16966\n",
      "Epoch: [0]  [410/622]  eta: 0:00:28  lr: 0.1  img/s: 2408.498940252397  loss: 5.9192 (6.3629)  acc1: 2.7344 (1.5532)  acc5: 9.3750 (5.4874)  time: 0.0918  data: 0.0395  max mem: 16966\n",
      "Epoch: [0]  [420/622]  eta: 0:00:26  lr: 0.1  img/s: 3030.912933452077  loss: 5.8614 (6.3425)  acc1: 2.7344 (1.6443)  acc5: 9.7656 (5.6777)  time: 0.0927  data: 0.0413  max mem: 16966\n",
      "Epoch: [0]  [430/622]  eta: 0:00:25  lr: 0.1  img/s: 2110.917490856389  loss: 5.8441 (6.3240)  acc1: 2.7344 (1.6779)  acc5: 10.5469 (5.8683)  time: 0.1002  data: 0.0546  max mem: 16966\n",
      "Epoch: [0]  [440/622]  eta: 0:00:23  lr: 0.1  img/s: 2323.6120469949924  loss: 5.7918 (6.3044)  acc1: 2.7344 (1.7535)  acc5: 10.5469 (6.1024)  time: 0.1130  data: 0.0703  max mem: 16966\n",
      "Epoch: [0]  [450/622]  eta: 0:00:22  lr: 0.1  img/s: 2431.6198912664154  loss: 5.7758 (6.2853)  acc1: 3.1250 (1.8512)  acc5: 11.3281 (6.2500)  time: 0.1050  data: 0.0635  max mem: 16966\n",
      "Epoch: [0]  [460/622]  eta: 0:00:21  lr: 0.1  img/s: 3050.051113422713  loss: 5.7286 (6.2633)  acc1: 3.1250 (1.9448)  acc5: 11.7188 (6.4910)  time: 0.0919  data: 0.0475  max mem: 16966\n",
      "Epoch: [0]  [470/622]  eta: 0:00:19  lr: 0.1  img/s: 2714.830569262903  loss: 5.5314 (6.2427)  acc1: 3.5156 (2.0589)  acc5: 12.1094 (6.7139)  time: 0.0864  data: 0.0403  max mem: 16966\n",
      "Epoch: [0]  [480/622]  eta: 0:00:18  lr: 0.1  img/s: 2607.0844864483993  loss: 5.5276 (6.2233)  acc1: 3.5156 (2.1923)  acc5: 12.5000 (6.9276)  time: 0.0935  data: 0.0487  max mem: 16966\n",
      "Epoch: [0]  [490/622]  eta: 0:00:16  lr: 0.1  img/s: 2673.644608111059  loss: 5.5198 (6.2070)  acc1: 3.9062 (2.2656)  acc5: 12.8906 (7.1484)  time: 0.0943  data: 0.0500  max mem: 16966\n",
      "Epoch: [0]  [500/622]  eta: 0:00:15  lr: 0.1  img/s: 3352.976666539677  loss: 5.5159 (6.1900)  acc1: 3.9062 (2.3667)  acc5: 12.8906 (7.3529)  time: 0.0833  data: 0.0405  max mem: 16966\n",
      "Epoch: [0]  [510/622]  eta: 0:00:14  lr: 0.1  img/s: 2582.536135938522  loss: 5.4965 (6.1737)  acc1: 4.2969 (2.4114)  acc5: 13.6719 (7.5646)  time: 0.0850  data: 0.0419  max mem: 16966\n",
      "Epoch: [0]  [520/622]  eta: 0:00:12  lr: 0.1  img/s: 2566.7180229319924  loss: 5.4821 (6.1604)  acc1: 4.6875 (2.4690)  acc5: 14.0625 (7.6946)  time: 0.0967  data: 0.0539  max mem: 16966\n",
      "Epoch: [0]  [530/622]  eta: 0:00:11  lr: 0.1  img/s: 3633.3280794191505  loss: 5.4670 (6.1458)  acc1: 5.0781 (2.5174)  acc5: 14.4531 (7.8559)  time: 0.0824  data: 0.0380  max mem: 16966\n",
      "Epoch: [0]  [540/622]  eta: 0:00:10  lr: 0.1  img/s: 5268.827948578792  loss: 5.4391 (6.1269)  acc1: 5.0781 (2.5994)  acc5: 14.8438 (8.1108)  time: 0.0568  data: 0.0120  max mem: 16966\n",
      "Epoch: [0]  [550/622]  eta: 0:00:08  lr: 0.1  img/s: 5298.454265664423  loss: 5.4279 (6.1127)  acc1: 5.4688 (2.6507)  acc5: 16.4062 (8.2729)  time: 0.0458  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [560/622]  eta: 0:00:07  lr: 0.1  img/s: 5227.8957480268955  loss: 5.4072 (6.0961)  acc1: 5.4688 (2.7618)  acc5: 16.4062 (8.5115)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [570/622]  eta: 0:00:06  lr: 0.1  img/s: 5281.303546308593  loss: 5.3751 (6.0820)  acc1: 5.4688 (2.8354)  acc5: 17.1875 (8.7150)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [580/622]  eta: 0:00:04  lr: 0.1  img/s: 5121.330912278744  loss: 5.3453 (6.0645)  acc1: 5.4688 (2.9396)  acc5: 17.1875 (8.9645)  time: 0.0466  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [590/622]  eta: 0:00:03  lr: 0.1  img/s: 5291.805950031172  loss: 5.3375 (6.0483)  acc1: 5.8594 (3.0859)  acc5: 17.1875 (9.1862)  time: 0.0465  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [600/622]  eta: 0:00:02  lr: 0.1  img/s: 5286.735640525827  loss: 5.3339 (6.0343)  acc1: 6.2500 (3.1762)  acc5: 17.1875 (9.3814)  time: 0.0458  data: 0.0015  max mem: 16966\n",
      "Epoch: [0]  [610/622]  eta: 0:00:01  lr: 0.1  img/s: 5235.686044663241  loss: 5.2932 (6.0187)  acc1: 6.2500 (3.2258)  acc5: 17.1875 (9.5010)  time: 0.0460  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [620/622]  eta: 0:00:00  lr: 0.1  img/s: 5294.193348618199  loss: 5.2774 (6.0045)  acc1: 6.2500 (3.2738)  acc5: 17.5781 (9.6726)  time: 0.0460  data: 0.0017  max mem: 16966\n",
      "Epoch: [0] Total time: 0:01:11\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "[2023-06-09 23:18:37] train.py:82\n",
      "    [context]:             image = image.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:37] train.py:83\n",
      "    [context]:             target = target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "Test:   [ 0/25]  eta: 0:02:25  loss: 7.6194 (7.6194)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.7812)  time: 5.8004  data: 0.0764  max mem: 16966\n",
      "Test:  Total time: 0:00:07\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=(6400,), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=(6400,), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([25, 195.1865692138672],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([25, 195.1865692138672],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 2200.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 2200.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 10400.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 10400.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "Test:  Acc@1 0.330 Acc@5 1.725\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "[2023-06-09 23:18:44] train.py:415\n",
      "    [context]:                 checkpoint[\"scaler\"] = scaler.state_dict()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.amp.GradScaler.state_dict() --> return state in diable mode\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 0:01:23\r\n"
     ]
    }
   ],
   "source": [
    "!GPU_MIGRATION_LOG_LEVEL=1 $PYTHON train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=/root/software/data/pytorch/imagenet/ILSVRC2012 --workers=8 --epochs=1 --opt=sgd --amp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
